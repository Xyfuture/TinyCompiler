# 设计思路&问题

## activation 层需要在本地执行
core执行完conv或者linear之后，需要在最后的几层上执行activation操作
建议这个东西直接嵌入到conv或者linear中，否则不是很好写
我们要设计一个核间的流水线，希望算出一个结果直接进行激活函数，不会像torch一样算完所有的结果在对整个tensor做激活函数

## conv的聚集 aggregate 
建议专门找一个核做聚集工作，将所有的结果收集起来并作激活函数。
要取得低延时应该是使用完成了matrix计算的核，要取得高吞吐应该是使用下一层的核，核间并行流水

## pooling的操作
应该在对应存储的激活的核上进行，可以按channel分开并行
